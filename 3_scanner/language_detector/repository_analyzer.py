# language_detector/repository_analyzer.py
import os
import re
from pathlib import Path
from typing import List
from models.file_metadata import (
    FileMetadata, LanguageStats, ScannerTargets, 
    RepositoryAnalysis, FileCategory
)
from .detector import LanguageDetector
from .file_classifier import FileClassifier
from .constants import IGNORE_DIRECTORIES, IGNORE_FILE_PATTERNS

class RepositoryAnalyzer:
    """Repository ì „ì²´ ë¶„ì„"""
    
    def __init__(self):
        self.detector = LanguageDetector()
        self.classifier = FileClassifier()
    
    def analyze(self, repo_path: str) -> RepositoryAnalysis:
        """Repository ë¶„ì„ ë©”ì¸"""
        print(f"ğŸ” Analyzing repository: {repo_path}")
        
        # 1. ëª¨ë“  íŒŒì¼ ìˆ˜ì§‘
        all_files = self._collect_files(repo_path)
        print(f"ğŸ“ Found {len(all_files)} files")
        
        # 2. ê° íŒŒì¼ ë¶„ì„
        file_metadata_list = []
        for file_path in all_files:
            metadata = self._analyze_file(file_path, repo_path)
            if metadata:
                file_metadata_list.append(metadata)
        
        print(f"âœ… Analyzed {len(file_metadata_list)} files")
        
        # 3. ì–¸ì–´ë³„ í†µê³„ ìƒì„±
        language_stats = self._generate_language_stats(file_metadata_list)
        
        # 4. ìŠ¤ìºë„ˆë³„ ë¶„ë¥˜
        scanner_targets = self._classify_for_scanners(file_metadata_list)
        
        print(f"ğŸ¯ SAST targets: {len(scanner_targets.sast_targets)}")
        print(f"ğŸ¯ SCA targets: {len(scanner_targets.sca_targets)}")
        print(f"ğŸ¯ Config targets: {len(scanner_targets.config_targets)}")
        
        return RepositoryAnalysis(
            repository_path=repo_path,
            total_files=len(file_metadata_list),
            file_metadata_list=file_metadata_list,
            language_stats=language_stats,
            scanner_targets=scanner_targets
        )
    
    def _collect_files(self, repo_path: str) -> List[str]:
        """ëª¨ë“  íŒŒì¼ ìˆ˜ì§‘"""
        files = []
        
        for root, dirs, filenames in os.walk(repo_path):
            # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ ì œì™¸
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRECTORIES]
            
            for filename in filenames:
                file_path = os.path.join(root, filename)
                
                # ë¬´ì‹œí•  íŒŒì¼ íŒ¨í„´ ì²´í¬
                if self._should_ignore_file(filename):
                    continue
                
                files.append(file_path)
        
        return files
    
    def _should_ignore_file(self, filename: str) -> bool:
        """íŒŒì¼ ë¬´ì‹œ ì—¬ë¶€"""
        for pattern in IGNORE_FILE_PATTERNS:
            if re.match(pattern, filename):
                return True
        return False
    
    def _analyze_file(
        self, 
        file_path: str, 
        repo_path: str
    ) -> FileMetadata:
        """ê°œë³„ íŒŒì¼ ë¶„ì„"""
        try:
            stat = os.stat(file_path)
            rel_path = os.path.relpath(file_path, repo_path)
            
            # ë°”ì´ë„ˆë¦¬ ì²´í¬
            is_binary = self._is_binary(file_path)
            
            # ë¼ì¸ ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ)
            line_count = 0
            encoding = 'utf-8'
            if not is_binary:
                line_count, encoding = self._count_lines(file_path)
            
            # ì–¸ì–´ ê°ì§€
            language = self.detector.detect_language(file_path)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = FileMetadata(
                file_path=rel_path,
                absolute_path=file_path,
                file_name=os.path.basename(file_path),
                extension=Path(file_path).suffix,
                language=language,
                category=FileCategory.UNKNOWN,  # ë‚˜ì¤‘ì— ë¶„ë¥˜
                size_bytes=stat.st_size,
                line_count=line_count,
                encoding=encoding,
                is_binary=is_binary
            )
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            metadata.category = self.classifier.classify(metadata)
            
            return metadata
        
        except Exception as e:
            print(f"âš ï¸  Error analyzing {file_path}: {e}")
            return None
    
    def _is_binary(self, file_path: str) -> bool:
        """ë°”ì´ë„ˆë¦¬ íŒŒì¼ ì²´í¬"""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\x00' in chunk
        except:
            return False
    
    def _count_lines(self, file_path: str) -> tuple[int, str]:
        """ë¼ì¸ ìˆ˜ ê³„ì‚° ë° ì¸ì½”ë”© ê°ì§€"""
        encodings = ['utf-8', 'latin-1', 'cp1252']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    lines = sum(1 for _ in f)
                return lines, encoding
            except:
                continue
        
        return 0, 'unknown'
    
    def _generate_language_stats(
        self, 
        file_metadata_list: List[FileMetadata]
    ) -> List[LanguageStats]:
        """ì–¸ì–´ë³„ í†µê³„ ìƒì„±"""
        stats_dict = {}
        
        for metadata in file_metadata_list:
            lang = metadata.language
            if lang not in stats_dict:
                stats_dict[lang] = {
                    'count': 0,
                    'lines': 0,
                    'bytes': 0
                }
            
            stats_dict[lang]['count'] += 1
            stats_dict[lang]['lines'] += metadata.line_count
            stats_dict[lang]['bytes'] += metadata.size_bytes
        
        # ì´ ë°”ì´íŠ¸ ê³„ì‚°
        total_bytes = sum(s['bytes'] for s in stats_dict.values())
        
        # LanguageStats ê°ì²´ ìƒì„±
        stats_list = []
        for lang, data in stats_dict.items():
            percentage = (data['bytes'] / total_bytes * 100) if total_bytes > 0 else 0
            
            stats_list.append(LanguageStats(
                language=lang,
                file_count=data['count'],
                total_lines=data['lines'],
                total_bytes=data['bytes'],
                percentage=round(percentage, 2)
            ))
        
        # í¼ì„¼í‹°ì§€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        stats_list.sort(key=lambda x: x.percentage, reverse=True)
        
        return stats_list
    
    def _classify_for_scanners(
        self, 
        file_metadata_list: List[FileMetadata]
    ) -> ScannerTargets:
        """ìŠ¤ìºë„ˆë³„ ëŒ€ìƒ ë¶„ë¥˜"""
        sast_targets = []
        sca_targets = []
        config_targets = []
        
        for metadata in file_metadata_list:
            if metadata.category == FileCategory.SOURCE_CODE:
                # ì†ŒìŠ¤ ì½”ë“œ â†’ SAST
                sast_targets.append(metadata)
            
            elif metadata.category == FileCategory.DEPENDENCY_MANIFEST:
                # ì˜ì¡´ì„± íŒŒì¼ â†’ SCA
                sca_targets.append(metadata)
            
            elif metadata.category == FileCategory.CONFIGURATION:
                # ì„¤ì • íŒŒì¼ â†’ Config (ì•”í˜¸ ê´€ë ¨ë§Œ)
                if self._is_crypto_related_config(metadata):
                    config_targets.append(metadata)
        
        return ScannerTargets(
            sast_targets=sast_targets,
            sca_targets=sca_targets,
            config_targets=config_targets
        )
    
    def _is_crypto_related_config(self, metadata: FileMetadata) -> bool:
        """ì•”í˜¸ ê´€ë ¨ ì„¤ì • íŒŒì¼ì¸ì§€ í™•ì¸"""
        # ì¸ì¦ì„œ íŒŒì¼
        if metadata.extension in ['.pem', '.crt', '.cer', '.key']:
            return True
        
        # TLS/SSL ê´€ë ¨ ì„¤ì •
        crypto_keywords = ['ssl', 'tls', 'cert', 'key', 'crypto', 'nginx', 'apache']
        path_lower = metadata.file_path.lower()
        
        return any(keyword in path_lower for keyword in crypto_keywords)